"""
ä½¿ç”¨LLMåˆ†æè®ºæ–‡ç›¸å…³æ€§ - ä¸¤é˜¶æ®µä¼˜åŒ–ç‰ˆæœ¬
ä½¿ç”¨OpenAIå®˜æ–¹Python SDK
"""
import os
import asyncio
import httpx
from typing import Dict, List, Tuple, Optional
from llm_client import LLMClient


class LLMAnalyzer:
    """ä½¿ç”¨LLMåˆ†æè®ºæ–‡ç›¸å…³æ€§ï¼ˆä¸¤é˜¶æ®µï¼šå¿«é€Ÿç­›é€‰ + è¯¦ç»†åˆ†æï¼‰"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4o",
        max_tokens: int = 4096,
        base_url: Optional[str] = None,
        max_concurrent: int = 5,
        batch_size: int = 25,
        detail_batch_size: int = 8
    ):
        """
        åˆå§‹åŒ–LLMåˆ†æå™¨

        Args:
            api_key: APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            base_url: è‡ªå®šä¹‰APIç«¯ç‚¹ (å¯é€‰)
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤5)
            batch_size: ç¬¬ä¸€é˜¶æ®µæ‰¹é‡ç­›é€‰æ—¶æ¯æ‰¹è®ºæ–‡æ•°é‡ (é»˜è®¤25)
            detail_batch_size: ç¬¬äºŒé˜¶æ®µæ‰¹é‡è¯¦ç»†åˆ†ææ—¶æ¯æ‰¹è®ºæ–‡æ•°é‡ (é»˜è®¤8)
        """
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        self.llm_client = LLMClient(
            api_key=api_key,
            base_url=base_url,
            model=model,
            max_tokens=max_tokens
        )

        self.max_concurrent = max_concurrent
        self.batch_size = batch_size
        self.detail_batch_size = detail_batch_size

    async def _call_api_async(self, prompt: str, client: httpx.AsyncClient, max_tokens: int = None) -> str:
        """
        å¼‚æ­¥è°ƒç”¨LLM API

        Args:
            prompt: æç¤ºè¯
            client: httpxå¼‚æ­¥å®¢æˆ·ç«¯
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            APIå“åº”æ–‡æœ¬
        """
        return await self.llm_client.chat_completion(
            prompt=prompt,
            client=client,
            max_tokens=max_tokens,
            temperature=0.7
        )

    async def _batch_filter_relevance_async(self, papers_batch: List[Dict], research_interests: List[str], client: httpx.AsyncClient, semaphore: asyncio.Semaphore, research_prompt: str = None) -> List[Tuple[int, str, List[str]]]:
        """
        æ‰¹é‡å¿«é€Ÿç­›é€‰è®ºæ–‡ç›¸å…³æ€§ï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰

        Args:
            papers_batch: ä¸€æ‰¹è®ºæ–‡
            research_interests: ç ”ç©¶æ–¹å‘åˆ—è¡¨
            client: httpxå¼‚æ­¥å®¢æˆ·ç«¯
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            research_prompt: ç ”ç©¶å…´è¶£çš„è¯¦ç»†æè¿°ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨ï¼‰

        Returns:
            [(è®ºæ–‡ç´¢å¼•, ç›¸å…³æ€§çº§åˆ«, åŒ¹é…é¢†åŸŸ), ...]
        """
        async with semaphore:
            # æ„å»ºæ‰¹é‡ç­›é€‰çš„æç¤ºè¯
            papers_text = ""
            for idx, paper in papers_batch:
                papers_text += f"\nã€è®ºæ–‡{idx}ã€‘\n"
                papers_text += f"æ ‡é¢˜: {paper['title']}\n"
                papers_text += f"æ‘˜è¦: {paper['abstract'][:800]}...\n"  # å¢åŠ æ‘˜è¦é•¿åº¦ä»¥è·å–æ›´å¤šä¿¡æ¯

            # æ ¹æ®æ˜¯å¦æä¾›äº† research_prompt æ¥æ„å»ºä¸åŒçš„ç”¨æˆ·ç ”ç©¶æ–¹å‘æè¿°
            if research_prompt:
                research_description = f"""ç”¨æˆ·çš„ç ”ç©¶å…´è¶£æè¿°ï¼š
{research_prompt}"""
            else:
                research_description = f"""ç”¨æˆ·çš„ç ”ç©¶æ–¹å‘ï¼š
{', '.join(research_interests)}"""

            prompt = f"""ä½ æ˜¯ä¸€ä¸ªAIç ”ç©¶åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹è®ºæ–‡æ˜¯å¦ä¸ç”¨æˆ·çš„ç ”ç©¶æ–¹å‘ç›¸å…³ã€‚

{research_description}

{papers_text}

è¯·å¯¹æ¯ç¯‡è®ºæ–‡æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼ˆåŠ¡å¿…åŒ…å«è®ºæ–‡ç¼–å·ï¼‰ï¼š

ã€è®ºæ–‡Xã€‘ç›¸å…³æ€§: é«˜/ä¸­/ä½/æ— å…³  |  åŒ¹é…é¢†åŸŸ: XXX, XXXï¼ˆå¦‚æœæ— å…³åˆ™å†™"æ— "ï¼‰

ç›¸å…³æ€§åˆ¤æ–­æ ‡å‡†ï¼š
- **é«˜ç›¸å…³**ï¼šè®ºæ–‡æ ¸å¿ƒå†…å®¹ç›´æ¥æœåŠ¡äºç”¨æˆ·çš„ç ”ç©¶æ–¹å‘ï¼Œæ–¹æ³•å’Œåº”ç”¨åœºæ™¯é«˜åº¦å¥‘åˆ
- **ä¸­ç›¸å…³**ï¼šè®ºæ–‡æ¶‰åŠç”¨æˆ·å…³æ³¨çš„æŠ€æœ¯æˆ–æ–¹æ³•ï¼Œè™½ç„¶åº”ç”¨åœºæ™¯ä¸å®Œå…¨ç›¸åŒï¼Œä½†æœ‰å€Ÿé‰´ä»·å€¼æˆ–æ½œåœ¨è¿ç§»å¯èƒ½
- **ä½ç›¸å…³**ï¼šè®ºæ–‡æåˆ°äº†ç›¸å…³çš„æ¦‚å¿µæˆ–æŠ€æœ¯ï¼Œä½†ä¸æ˜¯æ ¸å¿ƒå†…å®¹ï¼Œä»…æœ‰é—´æ¥è”ç³»
- **æ— å…³**ï¼šè®ºæ–‡å†…å®¹ä¸ç”¨æˆ·ç ”ç©¶æ–¹å‘å®Œå…¨æ— å…³

é‡è¦æç¤ºï¼š
- å¿…é¡»åŒ…å«ã€è®ºæ–‡Xã€‘æ ‡è®°
- é‡‡ç”¨**å®½æ¾çš„æ ‡å‡†**ï¼šåªè¦è®ºæ–‡æ¶‰åŠç›¸å…³æŠ€æœ¯ã€æ–¹æ³•æˆ–åº”ç”¨åœºæ™¯ï¼Œå³ä½¿ä¸æ˜¯å®Œå…¨åŒ¹é…ï¼Œä¹Ÿåº”æ ‡è®°ä¸º"ä¸­ç›¸å…³"æˆ–"é«˜ç›¸å…³"
- ç‰¹åˆ«å…³æ³¨ï¼šæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è§†è§‰ã€è¯­è¨€ã€å¤šæ¨¡æ€ã€å¼ºåŒ–å­¦ä¹ ã€ä¸–ç•Œæ¨¡å‹ç­‰ç›¸å…³è®ºæ–‡
- é¡¶çº§æœŸåˆŠï¼ˆNatureã€Scienceã€Cellç­‰ï¼‰çš„åˆ›æ–°æ–¹æ³•é€šå¸¸æœ‰è¿ç§»ä»·å€¼ï¼Œåº”ç»™äºˆæ›´é«˜è¯„åˆ†
- ä¸è¦è¿‡äºä¸¥æ ¼ï¼Œå®å¯å¤šç­›é€‰å‡ºä¸€äº›æ½œåœ¨ç›¸å…³çš„è®ºæ–‡"""

            try:
                response_text = await self._call_api_async(prompt, client, max_tokens=3072)

                # è§£ææ‰¹é‡å“åº”
                results = []
                lines = response_text.strip().split('\n')

                for line in lines:
                    line = line.strip()
                    if 'ã€è®ºæ–‡' in line and 'ã€‘' in line:
                        try:
                            # æå–è®ºæ–‡ç¼–å·
                            paper_idx = int(line.split('ã€è®ºæ–‡')[1].split('ã€‘')[0])

                            # æå–ç›¸å…³æ€§
                            relevance = 'none'
                            if 'ç›¸å…³æ€§' in line or 'ç›¸å…³åº¦' in line:
                                if 'é«˜' in line:
                                    relevance = 'high'
                                elif 'ä¸­' in line:
                                    relevance = 'medium'
                                elif 'ä½' in line:
                                    relevance = 'low'
                                elif 'æ— å…³' in line:
                                    relevance = 'none'

                            # æå–åŒ¹é…é¢†åŸŸ
                            matched = []
                            if 'åŒ¹é…é¢†åŸŸ' in line or 'ç›¸å…³é¢†åŸŸ' in line:
                                parts = line.split('åŒ¹é…é¢†åŸŸ:' if 'åŒ¹é…é¢†åŸŸ' in line else 'ç›¸å…³é¢†åŸŸ:')
                                if len(parts) > 1:
                                    fields_text = parts[1].strip()
                                    if fields_text and 'æ— ' not in fields_text:
                                        matched = [f.strip() for f in fields_text.replace('ã€', ',').split(',') if f.strip()]

                            results.append((paper_idx, relevance, matched))
                        except (ValueError, IndexError) as e:
                            print(f"  âš ï¸  è§£æè®ºæ–‡ç»“æœæ—¶å‡ºé”™: {line[:50]}... - {e}")
                            continue

                return results

            except Exception as e:
                import traceback
                error_msg = f"{type(e).__name__}: {str(e)}"
                print(f"  âš ï¸  æ‰¹é‡ç­›é€‰æ—¶å‡ºé”™: {error_msg}")
                print(f"  è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                # è¿”å›æ‰€æœ‰è®ºæ–‡æ ‡è®°ä¸ºunknown
                return [(idx, 'unknown', []) for idx, _ in papers_batch]

    async def _batch_analyze_detailed_async(self, papers_batch: List[Tuple[int, Dict]], client: httpx.AsyncClient, semaphore: asyncio.Semaphore) -> List[Tuple[int, Dict]]:
        """
        æ‰¹é‡è¯¦ç»†åˆ†æè®ºæ–‡ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰

        Args:
            papers_batch: ä¸€æ‰¹è®ºæ–‡ [(ç´¢å¼•, è®ºæ–‡), ...]
            client: httpxå¼‚æ­¥å®¢æˆ·ç«¯
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡

        Returns:
            [(è®ºæ–‡ç´¢å¼•, è¯¦ç»†åˆ†æç»“æœ), ...]
        """
        async with semaphore:
            # æ„å»ºæ‰¹é‡è¯¦ç»†åˆ†æçš„æç¤ºè¯
            papers_text = ""
            for idx, paper in papers_batch:
                authors_str = ', '.join(paper.get('authors', [])[:5])
                if len(paper.get('authors', [])) > 5:
                    authors_str += f' ç­‰ ({len(paper.get("authors", []))}ä½ä½œè€…)'

                papers_text += f"\n{'='*60}\n"
                papers_text += f"ã€è®ºæ–‡{idx}ã€‘\n"
                papers_text += f"æ ‡é¢˜ï¼š{paper['title']}\n"
                papers_text += f"ä½œè€…ï¼š{authors_str}\n"
                papers_text += f"æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰ï¼š{paper['abstract']}\n"

            prompt = f"""è¯·å¯¹ä»¥ä¸‹è®ºæ–‡è¿›è¡Œè¯¦ç»†åˆ†æã€‚

{papers_text}

è¯·å¯¹æ¯ç¯‡è®ºæ–‡æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼ˆåŠ¡å¿…åŒ…å«è®ºæ–‡ç¼–å·ï¼‰ï¼š

ã€è®ºæ–‡Xã€‘
1. ä½œè€…å•ä½ï¼šXXXï¼ˆå¦‚æœæ‘˜è¦ä¸­æåˆ°äº†ä½œè€…å•ä½ï¼Œè¯·åˆ—å‡ºï¼›å¦‚æœæ²¡æœ‰æåˆ°ï¼Œå†™"æœªåœ¨æ‘˜è¦ä¸­è¯´æ˜"ï¼‰
2. æ‘˜è¦ä¸­æ–‡ç¿»è¯‘ï¼šXXXï¼ˆå°†ä¸Šè¿°è‹±æ–‡æ‘˜è¦å®Œæ•´ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒå­¦æœ¯æ€§å’Œå‡†ç¡®æ€§ï¼‰
3. æ ¸å¿ƒå†…å®¹ï¼šXXXï¼ˆ1-2å¥è¯æ¦‚æ‹¬è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹å’Œè´¡çŒ®ï¼‰

æ³¨æ„ï¼š
- å¿…é¡»åŒ…å«ã€è®ºæ–‡Xã€‘æ ‡è®°
- ä½œè€…å•ä½åªä»æ‘˜è¦ä¸­æå–ï¼Œä¸è¦æ¨æµ‹
- ä¸­æ–‡ç¿»è¯‘è¦å®Œæ•´ã€å‡†ç¡®ã€æµç•…
- æ ¸å¿ƒå†…å®¹è¦çªå‡ºåˆ›æ–°ç‚¹"""

            try:
                response_text = await self._call_api_async(prompt, client, max_tokens=4096)

                # è§£ææ‰¹é‡å“åº”
                results = []
                current_paper_idx = None
                current_data = {'affiliations': None, 'abstract_zh': '', 'summary': ''}
                current_section = None
                current_content = []

                lines = response_text.strip().split('\n')

                for line in lines:
                    line_stripped = line.strip()

                    # æ£€æµ‹æ–°è®ºæ–‡å¼€å§‹
                    if 'ã€è®ºæ–‡' in line_stripped and 'ã€‘' in line_stripped:
                        # ä¿å­˜ä¸Šä¸€ç¯‡è®ºæ–‡çš„æ•°æ®
                        if current_paper_idx is not None:
                            if current_section and current_content:
                                content_text = '\n'.join(current_content).strip()
                                if current_section == 'abstract_zh':
                                    current_data['abstract_zh'] = content_text
                                elif current_section == 'summary':
                                    current_data['summary'] = content_text
                                elif current_section == 'affiliations' and 'æœªåœ¨æ‘˜è¦ä¸­è¯´æ˜' not in content_text:
                                    current_data['affiliations'] = content_text
                            results.append((current_paper_idx, current_data.copy()))

                        # å¼€å§‹æ–°è®ºæ–‡
                        try:
                            current_paper_idx = int(line_stripped.split('ã€è®ºæ–‡')[1].split('ã€‘')[0])
                            current_data = {'affiliations': None, 'abstract_zh': '', 'summary': ''}
                            current_section = None
                            current_content = []
                        except (ValueError, IndexError):
                            continue

                    # æ£€æµ‹ç« èŠ‚
                    elif current_paper_idx is not None:
                        if 'ä½œè€…å•ä½' in line_stripped or ('å•ä½' in line_stripped and ':' in line_stripped):
                            if current_section and current_content:
                                content_text = '\n'.join(current_content).strip()
                                if current_section == 'abstract_zh':
                                    current_data['abstract_zh'] = content_text
                                elif current_section == 'summary':
                                    current_data['summary'] = content_text
                            current_section = 'affiliations'
                            current_content = []
                            if 'ï¼š' in line_stripped or ':' in line_stripped:
                                separator = 'ï¼š' if 'ï¼š' in line_stripped else ':'
                                content = line_stripped.split(separator, 1)[-1].strip()
                                if content and 'æœªåœ¨æ‘˜è¦ä¸­è¯´æ˜' not in content:
                                    current_data['affiliations'] = content

                        elif 'æ‘˜è¦ä¸­æ–‡ç¿»è¯‘' in line_stripped or ('æ‘˜è¦' in line_stripped and 'ç¿»è¯‘' in line_stripped):
                            if current_section and current_content:
                                content_text = '\n'.join(current_content).strip()
                                if current_section == 'summary':
                                    current_data['summary'] = content_text
                                elif current_section == 'affiliations' and 'æœªåœ¨æ‘˜è¦ä¸­è¯´æ˜' not in content_text:
                                    current_data['affiliations'] = content_text
                            current_section = 'abstract_zh'
                            current_content = []
                            if 'ï¼š' in line_stripped:
                                content = line_stripped.split('ï¼š', 1)[-1].strip()
                                if content:
                                    current_content.append(content)

                        elif 'æ ¸å¿ƒå†…å®¹' in line_stripped or ('æ ¸å¿ƒ' in line_stripped and 'åˆ›æ–°' in line_stripped):
                            if current_section and current_content:
                                content_text = '\n'.join(current_content).strip()
                                if current_section == 'abstract_zh':
                                    current_data['abstract_zh'] = content_text
                                elif current_section == 'affiliations' and 'æœªåœ¨æ‘˜è¦ä¸­è¯´æ˜' not in content_text:
                                    current_data['affiliations'] = content_text
                            current_section = 'summary'
                            current_content = []
                            if 'ï¼š' in line_stripped:
                                content = line_stripped.split('ï¼š', 1)[-1].strip()
                                if content:
                                    current_content.append(content)

                        elif current_section and line_stripped and not line_stripped.startswith(('1.', '2.', '3.', 'æ³¨æ„', '=')):
                            current_content.append(line_stripped)

                # ä¿å­˜æœ€åä¸€ç¯‡è®ºæ–‡
                if current_paper_idx is not None:
                    if current_section and current_content:
                        content_text = '\n'.join(current_content).strip()
                        if current_section == 'abstract_zh':
                            current_data['abstract_zh'] = content_text
                        elif current_section == 'summary':
                            current_data['summary'] = content_text
                        elif current_section == 'affiliations' and 'æœªåœ¨æ‘˜è¦ä¸­è¯´æ˜' not in content_text:
                            current_data['affiliations'] = content_text
                    results.append((current_paper_idx, current_data))

                return results

            except Exception as e:
                import traceback
                error_msg = f"{type(e).__name__}: {str(e)}"
                print(f"  âš ï¸  æ‰¹é‡è¯¦ç»†åˆ†ææ—¶å‡ºé”™: {error_msg}")
                print(f"  è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                # è¿”å›æ‰€æœ‰è®ºæ–‡çš„ç©ºç»“æœ
                return [(idx, {'affiliations': None, 'abstract_zh': '', 'summary': '', 'reason': f'åˆ†æå¤±è´¥: {error_msg}'}) for idx, _ in papers_batch]


    async def two_stage_analyze_papers_async(self, papers: List[Dict], research_interests: List[str], research_prompt: str = None) -> List[Dict]:
        """
        ä¸¤é˜¶æ®µå¼‚æ­¥åˆ†æè®ºæ–‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

        ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡å¿«é€Ÿç­›é€‰ç›¸å…³æ€§ï¼ˆæ¯æ‰¹25ç¯‡ï¼Œåªåˆ¤æ–­ç›¸å…³æ€§ï¼‰
        ç¬¬äºŒé˜¶æ®µï¼šå¯¹ç›¸å…³è®ºæ–‡è¿›è¡Œè¯¦ç»†åˆ†æï¼ˆç¿»è¯‘ã€å•ä½ç­‰ï¼‰

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            research_interests: ç ”ç©¶æ–¹å‘åˆ—è¡¨
            research_prompt: ç ”ç©¶å…´è¶£çš„è¯¦ç»†æè¿°ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨ï¼‰

        Returns:
            å¸¦æœ‰åˆ†æç»“æœçš„è®ºæ–‡åˆ—è¡¨
        """
        total = len(papers)
        print(f"\n{'='*60}")
        print(f"ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡å¿«é€Ÿç­›é€‰ {total} ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§")
        print(f"   - æ‰¹æ¬¡å¤§å°: {self.batch_size} ç¯‡/æ‰¹")
        print(f"   - å¹¶å‘æ•°: {self.max_concurrent}")
        if research_prompt:
            print(f"   - ä½¿ç”¨æ¨¡å¼: è‡ªå®šä¹‰ç ”ç©¶å…´è¶£æè¿°")
        else:
            print(f"   - ä½¿ç”¨æ¨¡å¼: å…³é”®è¯åˆ—è¡¨")
        print(f"{'='*60}")

        # ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡ç­›é€‰ç›¸å…³æ€§
        semaphore = asyncio.Semaphore(self.max_concurrent)
        all_papers_with_relevance = papers.copy()

        async with httpx.AsyncClient(
            limits=httpx.Limits(max_connections=self.max_concurrent * 2, max_keepalive_connections=self.max_concurrent),
            timeout=httpx.Timeout(60.0, connect=10.0)
        ) as client:
            # å°†è®ºæ–‡åˆ†æ‰¹
            batches = []
            for i in range(0, total, self.batch_size):
                batch = [(j, papers[j]) for j in range(i, min(i + self.batch_size, total))]
                batches.append(batch)

            print(f"åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œç­›é€‰...\n")

            # å¹¶å‘å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
            tasks = [
                self._batch_filter_relevance_async(batch, research_interests, client, semaphore, research_prompt)
                for batch in batches
            ]

            batch_results = []
            for i, task in enumerate(asyncio.as_completed(tasks), 1):
                result = await task
                batch_results.extend(result)
                print(f"  [{i}/{len(batches)}] âœ“ å®Œæˆæ‰¹æ¬¡ {i}")

            # åˆå¹¶ç­›é€‰ç»“æœåˆ°è®ºæ–‡æ•°æ®
            for paper_idx, relevance, matched in batch_results:
                if 0 <= paper_idx < len(all_papers_with_relevance):
                    all_papers_with_relevance[paper_idx]['relevance_level'] = relevance
                    all_papers_with_relevance[paper_idx]['matched_interests'] = matched
                    all_papers_with_relevance[paper_idx]['is_relevant'] = relevance in ['high', 'medium']

            # ç»Ÿè®¡ç›¸å…³è®ºæ–‡
            relevant_papers = [p for p in all_papers_with_relevance if p.get('is_relevant', False)]
            print(f"\nâœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼ç­›é€‰å‡º {len(relevant_papers)}/{total} ç¯‡ç›¸å…³è®ºæ–‡")

            if not relevant_papers:
                print("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ï¼Œè·³è¿‡ç¬¬äºŒé˜¶æ®µ")
                return all_papers_with_relevance

            # ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è¯¦ç»†åˆ†æç›¸å…³è®ºæ–‡
            print(f"\n{'='*60}")
            print(f"ğŸ” ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è¯¦ç»†åˆ†æ {len(relevant_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
            print(f"   - æ‰¹æ¬¡å¤§å°: {self.detail_batch_size} ç¯‡/æ‰¹")
            print(f"{'='*60}\n")

            # ä¸ºæ¯ç¯‡è®ºæ–‡åˆ›å»ºç´¢å¼•æ˜ å°„
            paper_index_map = {id(paper): i for i, paper in enumerate(relevant_papers)}

            # å°†ç›¸å…³è®ºæ–‡åˆ†æ‰¹
            detail_batches = []
            for i in range(0, len(relevant_papers), self.detail_batch_size):
                batch = [(paper_index_map[id(relevant_papers[j])], relevant_papers[j])
                         for j in range(i, min(i + self.detail_batch_size, len(relevant_papers)))]
                detail_batches.append(batch)

            print(f"åˆ†ä¸º {len(detail_batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œè¯¦ç»†åˆ†æ...\n")

            # å¹¶å‘å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
            detail_tasks = [
                self._batch_analyze_detailed_async(batch, client, semaphore)
                for batch in detail_batches
            ]

            all_details = []
            for i, task in enumerate(asyncio.as_completed(detail_tasks), 1):
                batch_details = await task
                all_details.extend(batch_details)
                print(f"  [{i}/{len(detail_batches)}] âœ“ å®Œæˆæ‰¹æ¬¡ {i} ({len(batch_details)} ç¯‡)")

            # æ›´æ–°è®ºæ–‡è¯¦ç»†ä¿¡æ¯
            for paper_idx, details in all_details:
                if 0 <= paper_idx < len(relevant_papers):
                    relevant_papers[paper_idx].update(details)

        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"   - æ€»è®ºæ–‡æ•°: {total}")
        print(f"   - ç›¸å…³è®ºæ–‡: {len(relevant_papers)}")
        print(f"   - é«˜ç›¸å…³: {sum(1 for p in all_papers_with_relevance if p.get('relevance_level') == 'high')}")
        print(f"   - ä¸­ç›¸å…³: {sum(1 for p in all_papers_with_relevance if p.get('relevance_level') == 'medium')}")

        return all_papers_with_relevance

    def filter_relevant_papers(self, analyzed_papers: List[Dict], min_relevance: str = 'medium') -> List[Dict]:
        """
        è¿‡æ»¤å‡ºç›¸å…³çš„è®ºæ–‡

        Args:
            analyzed_papers: å·²åˆ†æçš„è®ºæ–‡åˆ—è¡¨
            min_relevance: æœ€å°ç›¸å…³æ€§çº§åˆ« ('high', 'medium', 'low')

        Returns:
            ç›¸å…³è®ºæ–‡åˆ—è¡¨
        """
        relevance_order = {'high': 3, 'medium': 2, 'low': 1, 'none': 0, 'unknown': 0}
        min_level = relevance_order.get(min_relevance, 2)

        relevant_papers = [
            paper for paper in analyzed_papers
            if relevance_order.get(paper.get('relevance_level', 'unknown'), 0) >= min_level
        ]

        # æŒ‰ç›¸å…³æ€§æ’åº
        relevant_papers.sort(
            key=lambda x: relevance_order.get(x.get('relevance_level', 'unknown'), 0),
            reverse=True
        )

        return relevant_papers
