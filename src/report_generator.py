"""
ç”ŸæˆMarkdownæ ¼å¼çš„è®ºæ–‡æŠ¥å‘Š
"""
import os
from datetime import datetime
from typing import List, Dict


class ReportGenerator:
    """MarkdownæŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, output_dir: str = "reports"):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

    def generate_report(self, papers: List[Dict], research_interests: List[str], tweets: List[Dict] = None) -> str:
        """
        ç”Ÿæˆå¤šæºå†…å®¹æŠ¥å‘Š

        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆå·²åŒ…å«åˆ†æç»“æœï¼‰
            research_interests: ç ”ç©¶æ–¹å‘åˆ—è¡¨
            tweets: æ¨æ–‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # ç”Ÿæˆæ–‡ä»¶å
        date_str = datetime.now().strftime('%Y-%m-%d')
        filename = f"arxiv_papers_{date_str}.md"
        filepath = os.path.join(self.output_dir, filename)

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        content = self._generate_content(papers, research_interests, date_str, tweets or [])

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
        return filepath

    def generate_html_report(self, papers: List[Dict], research_interests: List[str], tweets: List[Dict] = None) -> str:
        """
        ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Šï¼ˆç”¨äºé‚®ä»¶ï¼‰

        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆå·²åŒ…å«åˆ†æç»“æœï¼‰
            research_interests: ç ”ç©¶æ–¹å‘åˆ—è¡¨
            tweets: æ¨æ–‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            HTMLå†…å®¹å­—ç¬¦ä¸²
        """
        date_str = datetime.now().strftime('%Y-%m-%d')
        tweets = tweets or []

        # ç”ŸæˆHTMLå†…å®¹
        html_content = self._generate_html_content(papers, research_interests, date_str, tweets)

        return html_content

    def _generate_content(self, papers: List[Dict], research_interests: List[str], date_str: str, tweets: List[Dict] = None) -> str:
        """
        ç”ŸæˆæŠ¥å‘Šå†…å®¹

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            research_interests: ç ”ç©¶æ–¹å‘åˆ—è¡¨
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            tweets: æ¨æ–‡åˆ—è¡¨

        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Šå†…å®¹
        """
        tweets = tweets or []
        lines = []

        # æ ‡é¢˜
        lines.append(f"# å­¦æœ¯å†…å®¹æ—¥æŠ¥ - {date_str}\n")

        # ç ”ç©¶æ–¹å‘
        lines.append("## ç ”ç©¶æ–¹å‘\n")
        for interest in research_interests:
            lines.append(f"- {interest}")
        lines.append("")

        # ç»Ÿè®¡ä¿¡æ¯
        lines.append("## ç»Ÿè®¡ä¿¡æ¯\n")
        lines.append(f"- æ€»è®ºæ–‡/æ–‡ç« æ•°: {len(papers)}")
        if tweets:
            lines.append(f"- æ€»æ¨æ–‡æ•°: {len(tweets)}")

        # æŒ‰ç›¸å…³æ€§çº§åˆ«ç»Ÿè®¡ï¼ˆä»…å½“æœ‰åˆ†æç»“æœæ—¶æ˜¾ç¤ºï¼‰
        relevance_stats = {}
        for paper in papers:
            level = paper.get('relevance_level', 'unknown')
            if level != 'unknown':  # åªç»Ÿè®¡å·²åˆ†æçš„è®ºæ–‡
                relevance_stats[level] = relevance_stats.get(level, 0) + 1

        if relevance_stats:  # åªæœ‰å½“æœ‰å·²åˆ†æçš„è®ºæ–‡æ—¶æ‰æ˜¾ç¤ºç›¸å…³æ€§åˆ†å¸ƒ
            lines.append("- è®ºæ–‡ç›¸å…³æ€§åˆ†å¸ƒ:")
            for level in ['high', 'medium', 'low', 'none']:
                if level in relevance_stats:
                    lines.append(f"  - {level}: {relevance_stats[level]}")

        # æ¨æ–‡ç›¸å…³æ€§ç»Ÿè®¡
        if tweets:
            tweet_stats = {}
            for tweet in tweets:
                level = tweet.get('relevance_level', 'unknown')
                if level != 'unknown':
                    tweet_stats[level] = tweet_stats.get(level, 0) + 1
            if tweet_stats:
                lines.append("- æ¨æ–‡ç›¸å…³æ€§åˆ†å¸ƒ:")
                for level in ['high', 'medium', 'low']:
                    if level in tweet_stats:
                        lines.append(f"  - {level}: {tweet_stats[level]}")

        lines.append("")

        # ç”Ÿæˆæ ¸å¿ƒå‘ç°æ‘˜è¦ï¼ˆè®ºæ–‡+æ¨æ–‡ï¼‰
        high_papers = [p for p in papers if p.get('relevance_level') == 'high']
        high_tweets = [t for t in tweets if t.get('relevance_level') == 'high']

        if high_papers or high_tweets:
            lines.append("## ğŸ” æ ¸å¿ƒå‘ç°é€Ÿè§ˆ\n")

            # è®ºæ–‡æ‘˜è¦
            if high_papers:
                summary_by_topic = self._generate_topic_summary(high_papers)
                lines.extend(summary_by_topic)

            # Twitterçƒ­ç‚¹æ‘˜è¦
            if high_tweets:
                lines.append("")
                twitter_summary = self._generate_twitter_summary(high_tweets)
                lines.extend(twitter_summary)

            lines.append("")

        # æŒ‰ç›¸å…³æ€§åˆ†ç»„
        medium_papers = [p for p in papers if p.get('relevance_level') == 'medium']
        low_papers = [p for p in papers if p.get('relevance_level') == 'low']
        unanalyzed_papers = [p for p in papers if p.get('relevance_level') in ['unknown', None]]

        # é«˜ç›¸å…³æ€§è®ºæ–‡
        if high_papers:
            lines.append("## å¼ºçƒˆæ¨è (é«˜ç›¸å…³æ€§)\n")
            for paper in high_papers:
                lines.extend(self._format_paper(paper))

        # ä¸­ç­‰ç›¸å…³æ€§è®ºæ–‡
        if medium_papers:
            lines.append("## æ¨èé˜…è¯» (ä¸­ç­‰ç›¸å…³æ€§)\n")
            for paper in medium_papers:
                lines.extend(self._format_paper(paper))

        # ä½ç›¸å…³æ€§è®ºæ–‡
        if low_papers:
            lines.append("## å¯èƒ½æ„Ÿå…´è¶£ (ä½ç›¸å…³æ€§)\n")
            for paper in low_papers:
                lines.extend(self._format_paper(paper))

        # æœªåˆ†æçš„è®ºæ–‡ï¼ˆæŒ‰æ—¥æœŸæ’åºï¼‰
        if unanalyzed_papers:
            lines.append("## æ‰€æœ‰è®ºæ–‡ (æœªåˆ†æ)\n")
            # æŒ‰æ›´æ–°æ—¥æœŸæˆ–å‘å¸ƒæ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            unanalyzed_papers.sort(key=lambda x: x.get('updated', x.get('published', '')), reverse=True)
            for paper in unanalyzed_papers:
                lines.extend(self._format_paper(paper))

        # Twitter æ¨æ–‡éƒ¨åˆ†
        if tweets:
            lines.append("---\n")
            lines.append("# Twitter å­¦æœ¯åŠ¨æ€\n")

            # æŒ‰ç›¸å…³æ€§åˆ†ç»„
            high_tweets = [t for t in tweets if t.get('relevance_level') == 'high']
            medium_tweets = [t for t in tweets if t.get('relevance_level') == 'medium']
            low_tweets = [t for t in tweets if t.get('relevance_level') == 'low']

            # é«˜ç›¸å…³æ¨æ–‡
            if high_tweets:
                lines.append("## å¼ºçƒˆæ¨è (é«˜ç›¸å…³æ€§)\n")
                for tweet in high_tweets:
                    lines.extend(self._format_tweet(tweet))

            # ä¸­ç­‰ç›¸å…³æ¨æ–‡
            if medium_tweets:
                lines.append("## å€¼å¾—å…³æ³¨ (ä¸­ç­‰ç›¸å…³æ€§)\n")
                for tweet in medium_tweets:
                    lines.extend(self._format_tweet(tweet))

            # ä½ç›¸å…³æ¨æ–‡
            if low_tweets:
                lines.append("## å¯èƒ½æ„Ÿå…´è¶£ (ä½ç›¸å…³æ€§)\n")
                for tweet in low_tweets:
                    lines.extend(self._format_tweet(tweet))

        return '\n'.join(lines)

    def _generate_topic_summary(self, papers: List[Dict]) -> List[str]:
        """
        ç”ŸæˆåŠ©æ‰‹é£æ ¼çš„æ ¸å¿ƒå‘ç°æ‘˜è¦

        Args:
            papers: é«˜ç›¸å…³è®ºæ–‡åˆ—è¡¨

        Returns:
            æ‘˜è¦æ–‡æœ¬è¡Œåˆ—è¡¨
        """
        lines = []

        # æŒ‰ä¸»é¢˜åˆ†ç±»ï¼ˆåŸºäºmatched_interestså­—æ®µï¼‰
        topic_papers = {}
        for paper in papers:
            interests = paper.get('matched_interests', [])
            source = paper.get('journal', paper.get('primary_category', 'ArXiv'))

            # å¦‚æœæ˜¯CNSæœŸåˆŠï¼Œæ·»åŠ æœŸåˆŠæ ‡è®°
            is_cns = any(journal in str(source) for journal in ['Nature', 'Science', 'Cell'])

            if interests:
                for interest in interests:
                    if interest not in topic_papers:
                        topic_papers[interest] = []
                    topic_papers[interest].append((paper, is_cns))
            else:
                # æ²¡æœ‰æ˜ç¡®ä¸»é¢˜çš„å½’ç±»ä¸º"å…¶ä»–"
                if 'å…¶ä»–å‰æ²¿ç ”ç©¶' not in topic_papers:
                    topic_papers['å…¶ä»–å‰æ²¿ç ”ç©¶'] = []
                topic_papers['å…¶ä»–å‰æ²¿ç ”ç©¶'].append((paper, is_cns))

        # ç”ŸæˆåŠ©æ‰‹é£æ ¼çš„æ€»ç»“
        lines.append("æ ¹æ®æ‚¨çš„ç ”ç©¶å…´è¶£ï¼Œæœ¬æœŸä¸ºæ‚¨ç­›é€‰å‡º **{}ç¯‡é«˜ç›¸å…³è®ºæ–‡**ã€‚ä»¥ä¸‹æ˜¯æ ¸å¿ƒå‘ç°ï¼š\n".format(len(papers)))

        # æŒ‰ä¸»é¢˜è¾“å‡ºï¼Œæ¯ä¸ªä¸»é¢˜æœ€å¤šæ¨è2-3ç¯‡
        total_recommended = 0
        max_total = 10  # æ€»å…±ä¸è¶…è¿‡10ç¯‡

        for topic, topic_papers_list in sorted(topic_papers.items(), key=lambda x: -len(x[1])):
            if total_recommended >= max_total:
                break

            # ä¼˜å…ˆæ˜¾ç¤ºCNSæœŸåˆŠæ–‡ç« 
            topic_papers_list.sort(key=lambda x: (not x[1], x[0].get('title', '')))

            # æ¯ä¸ªä¸»é¢˜æ¨èçš„æ•°é‡
            recommend_count = min(3, len(topic_papers_list), max_total - total_recommended)
            if recommend_count == 0:
                continue

            lines.append(f"**{topic}**ï¼šæœ¬æœŸæ‰¾åˆ°{len(topic_papers_list)}ç¯‡ç›¸å…³è®ºæ–‡ï¼Œä¸ºæ‚¨é‡ç‚¹æ¨èä»¥ä¸‹{recommend_count}ç¯‡ï¼š\n")

            for i, (paper, is_cns) in enumerate(topic_papers_list[:recommend_count], 1):
                total_recommended += 1
                title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
                source = paper.get('journal') if paper.get('journal') else f"ArXiv {paper.get('primary_category', 'unknown')}"

                # ç”Ÿæˆç¬¦åˆGitHub MDè§„èŒƒçš„é”šç‚¹ï¼ˆåªä¿ç•™å­—æ¯æ•°å­—å’Œè¿å­—ç¬¦ï¼‰
                import re
                anchor = re.sub(r'[^\w\s-]', '', title.lower())  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                anchor = re.sub(r'[\s_]+', '-', anchor)  # ç©ºæ ¼å’Œä¸‹åˆ’çº¿è½¬ä¸ºè¿å­—ç¬¦
                anchor = anchor[:80]  # é™åˆ¶é•¿åº¦

                # CNSæœŸåˆŠç‰¹åˆ«æ ‡æ³¨
                source_tag = f"**{source}**" if is_cns else source

                # ç®€çŸ­è¯´æ˜ï¼ˆä»summaryæˆ–why_relevantæå–ï¼‰
                brief = paper.get('summary', paper.get('why_relevant', ''))
                if brief:
                    # æå–ç¬¬ä¸€å¥è¯æˆ–å‰100å­—
                    brief = brief.split('ã€‚')[0].split('.')[0][:100].replace('\n', ' ').strip()

                lines.append(f"{i}. [{title}](#{anchor}) ({source_tag})")
                if brief:
                    lines.append(f"   - {brief}")
                lines.append("")

            # å¦‚æœè¿˜æœ‰æ›´å¤šè®ºæ–‡
            remaining = len(topic_papers_list) - recommend_count
            if remaining > 0:
                lines.append(f"   *å¦æœ‰{remaining}ç¯‡{topic}ç›¸å…³è®ºæ–‡ï¼Œè¯¦è§ä¸‹æ–‡*\n")

        if total_recommended < len(papers):
            lines.append(f"\nå…¶ä½™ {len(papers) - total_recommended} ç¯‡é«˜ç›¸å…³è®ºæ–‡è¯¦è§ä¸‹æ–‡ã€Œå¼ºçƒˆæ¨èã€éƒ¨åˆ†ã€‚\n")

        return lines

    def _generate_twitter_summary(self, tweets: List[Dict]) -> List[str]:
        """
        ç”ŸæˆTwitterçƒ­ç‚¹æ‘˜è¦

        Args:
            tweets: é«˜ç›¸å…³æ¨æ–‡åˆ—è¡¨

        Returns:
            æ‘˜è¦æ–‡æœ¬è¡Œåˆ—è¡¨
        """
        lines = []

        lines.append("**ğŸ“± Twitterå­¦æœ¯åŠ¨æ€**\n")

        # åˆ†ææ¨æ–‡ä¸»é¢˜
        topics = {}
        for tweet in tweets:
            text = tweet.get('text', '')
            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆåŸºäºå¸¸è§å­¦æœ¯è¯é¢˜ï¼‰
            keywords = []
            topic_keywords = {
                'è‡ªåŠ¨é©¾é©¶': ['è‡ªåŠ¨é©¾é©¶', 'autonomous driving', 'self-driving', 'Tesla', 'FSD'],
                'å¤§è¯­è¨€æ¨¡å‹': ['LLM', 'GPT', 'Claude', 'language model', 'å¤§è¯­è¨€æ¨¡å‹', 'å¤§æ¨¡å‹'],
                'å¼ºåŒ–å­¦ä¹ ': ['å¼ºåŒ–å­¦ä¹ ', 'reinforcement learning', 'RL', 'policy'],
                'å…·èº«æ™ºèƒ½': ['å…·èº«', 'embodied', 'robot', 'æœºå™¨äºº', 'manipulation'],
                'VLM/å¤šæ¨¡æ€': ['VLM', 'vision language', 'å¤šæ¨¡æ€', 'multimodal', 'CLIP'],
            }

            for topic, kws in topic_keywords.items():
                if any(kw.lower() in text.lower() for kw in kws):
                    if topic not in topics:
                        topics[topic] = []
                    topics[topic].append(tweet)
                    break

        # ç”Ÿæˆæ€»ç»“
        if not topics:
            lines.append(f"æœ¬æœŸæ”¶é›†äº†{len(tweets)}æ¡ä¸æ‚¨ç ”ç©¶æ–¹å‘ç›¸å…³çš„å­¦æœ¯è®¨è®ºï¼Œæ¶µç›–äº†æœ€æ–°çš„æŠ€æœ¯åŠ¨æ€å’Œç ”ç©¶è¿›å±•ã€‚")
        else:
            topic_summary = []
            for topic, topic_tweets in sorted(topics.items(), key=lambda x: -len(x[1]))[:3]:
                topic_summary.append(f"**{topic}**ï¼ˆ{len(topic_tweets)}æ¡ï¼‰")

            lines.append(f"æœ¬æœŸTwitterå­¦æœ¯åœˆçš„çƒ­ç‚¹è¯é¢˜åŒ…æ‹¬ï¼š{' | '.join(topic_summary)}\n")

            # æŒ‘é€‰2-3æ¡æœ‰ä»£è¡¨æ€§çš„æ¨æ–‡
            sample_count = min(3, len(tweets))
            lines.append(f"ä¸ºæ‚¨ç²¾é€‰{sample_count}æ¡æœ€å…·ä»£è¡¨æ€§çš„è®¨è®ºï¼š\n")

            for i, tweet in enumerate(tweets[:sample_count], 1):
                author = tweet.get('author_name', tweet.get('author_username', 'æœªçŸ¥'))
                text_preview = tweet.get('text', '')[:100].replace('\n', ' ').strip()
                if len(tweet.get('text', '')) > 100:
                    text_preview += '...'

                lines.append(f"{i}. **@{author}**ï¼š{text_preview}")

                # æ·»åŠ ç›¸å…³æ€§è¯´æ˜
                if tweet.get('why_relevant'):
                    relevance = tweet.get('why_relevant', '')[:80].replace('\n', ' ')
                    lines.append(f"   > {relevance}")

                lines.append("")

        lines.append(f"*å®Œæ•´TwitteråŠ¨æ€è¯¦è§ä¸‹æ–‡ã€ŒTwitterå­¦æœ¯åŠ¨æ€ã€éƒ¨åˆ†*\n")

        return lines

    def _format_paper(self, paper: Dict) -> List[str]:
        """
        æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡

        Args:
            paper: è®ºæ–‡ä¿¡æ¯

        Returns:
            æ ¼å¼åŒ–åçš„è¡Œåˆ—è¡¨
        """
        lines = []

        # æ ‡é¢˜
        lines.append(f"### {paper.get('title', 'æ— æ ‡é¢˜')}\n")

        # åŸºæœ¬ä¿¡æ¯
        published_date = paper.get('published') or paper.get('pub_date') or paper.get('date', 'æœªçŸ¥')
        updated_date = paper.get('updated')
        if updated_date and updated_date != published_date:
            lines.append(f"**å‘å¸ƒæ—¥æœŸ:** {published_date} (æ›´æ–°: {updated_date})")
        else:
            lines.append(f"**å‘å¸ƒæ—¥æœŸ:** {published_date}")

        # ä½œè€…å’Œå•ä½
        authors = paper.get('authors', [])
        if authors:
            authors_display = ', '.join(authors[:3])
            if len(authors) > 3:
                authors_display += f" ç­‰ ({len(authors)}ä½ä½œè€…)"
            lines.append(f"**ä½œè€…:** {authors_display}")

        # ä½œè€…å•ä½ï¼ˆå¦‚æœæœ‰ï¼‰
        if paper.get('affiliations'):
            lines.append(f"**å•ä½:** {paper['affiliations']}")

        # ç±»åˆ«/æœŸåˆŠï¼ˆArXivæœ‰categoryï¼ŒæœŸåˆŠæœ‰journalï¼‰
        if paper.get('primary_category'):
            lines.append(f"**ç±»åˆ«:** {paper['primary_category']}")
        elif paper.get('journal'):
            lines.append(f"**æœŸåˆŠ:** {paper['journal']}")

        # é“¾æ¥
        if paper.get('url'):
            lines.append(f"**è®ºæ–‡é“¾æ¥:** {paper['url']}")
        if paper.get('pdf_url'):
            lines.append(f"**PDFé“¾æ¥:** {paper['pdf_url']}")

        # ç›¸å…³æ€§ä¿¡æ¯
        if paper.get('matched_interests'):
            lines.append(f"**ç›¸å…³é¢†åŸŸ:** {', '.join(paper['matched_interests'])}")

        # æ‘˜è¦ä¸­æ–‡ç¿»è¯‘ï¼ˆä¼˜å…ˆæ˜¾ç¤ºï¼‰
        if paper.get('abstract_zh'):
            lines.append(f"\n**æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰:**")
            lines.append(f"{paper['abstract_zh']}")

        # AIç”Ÿæˆçš„æ€»ç»“ï¼ˆå¦‚æœæœ‰ï¼‰
        if paper.get('summary'):
            lines.append(f"\n**æ ¸å¿ƒå†…å®¹:**")
            lines.append(f"{paper['summary']}")

        lines.append("\n---\n")

        return lines

    def _format_tweet(self, tweet: Dict) -> List[str]:
        """
        æ ¼å¼åŒ–å•æ¡æ¨æ–‡

        Args:
            tweet: æ¨æ–‡ä¿¡æ¯

        Returns:
            æ ¼å¼åŒ–åçš„è¡Œåˆ—è¡¨
        """
        lines = []

        # ä½œè€…ä¿¡æ¯
        author = f"@{tweet.get('author_username', 'unknown')}"
        if tweet.get('author_name'):
            author = f"{tweet.get('author_name')} ({author})"

        lines.append(f"### {author}\n")

        # å‘å¸ƒæ—¶é—´
        lines.append(f"**å‘å¸ƒæ—¶é—´:** {tweet.get('created_at', 'unknown')}")

        # æ¨æ–‡å†…å®¹
        lines.append(f"\n**å†…å®¹:**")
        lines.append(f"{tweet.get('text', '')}")

        # AIåˆ†æçš„ç›¸å…³æ€§è¯´æ˜
        if tweet.get('why_relevant'):
            lines.append(f"\n**ç›¸å…³æ€§è¯´æ˜:**")
            lines.append(f"{tweet.get('why_relevant')}")

        # äº’åŠ¨æ•°æ®
        if tweet.get('favorite_count') or tweet.get('retweet_count'):
            lines.append(f"\n**äº’åŠ¨æ•°æ®:**")
            lines.append(f"ğŸ‘ {tweet.get('favorite_count', 0)} | ğŸ”„ {tweet.get('retweet_count', 0)} | ğŸ’¬ {tweet.get('reply_count', 0)}")

        # æ¨æ–‡é“¾æ¥
        lines.append(f"\n**é“¾æ¥:** {tweet.get('url', '')}")

        lines.append("\n---\n")

        return lines

    def generate_simple_list(self, papers: List[Dict]) -> str:
        """
        ç”Ÿæˆç®€å•çš„è®ºæ–‡åˆ—è¡¨ï¼ˆç”¨äºå¿«é€Ÿæµè§ˆï¼‰

        Args:
            papers: è®ºæ–‡åˆ—è¡¨

        Returns:
            ç®€å•åˆ—è¡¨çš„æ–‡æœ¬
        """
        lines = []

        for i, paper in enumerate(papers, 1):
            lines.append(f"{i}. **{paper['title']}**")
            lines.append(f"   - URL: {paper['url']}")
            lines.append(f"   - ç›¸å…³æ€§: {paper.get('relevance_level', 'unknown')}")
            if paper.get('matched_interests'):
                lines.append(f"   - ç›¸å…³é¢†åŸŸ: {', '.join(paper['matched_interests'])}")
            lines.append("")

        return '\n'.join(lines)

    def _generate_paper_anchor(self, paper: Dict, index: int = 0) -> str:
        """
        ç”Ÿæˆè®ºæ–‡çš„å”¯ä¸€é”šç‚¹ID
        ä½¿ç”¨è®ºæ–‡URLçš„hashæˆ–ç´¢å¼•ï¼Œç¡®ä¿å”¯ä¸€æ€§å’Œä¸€è‡´æ€§

        Args:
            paper: è®ºæ–‡ä¿¡æ¯
            index: è®ºæ–‡ç´¢å¼•ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰

        Returns:
            é”šç‚¹IDå­—ç¬¦ä¸²
        """
        import hashlib

        # ä¼˜å…ˆä½¿ç”¨è®ºæ–‡URLç”Ÿæˆhashä½œä¸ºé”šç‚¹
        if paper.get('url'):
            url_hash = hashlib.md5(paper['url'].encode()).hexdigest()[:12]
            return f"paper-{url_hash}"
        elif paper.get('pdf_url'):
            url_hash = hashlib.md5(paper['pdf_url'].encode()).hexdigest()[:12]
            return f"paper-{url_hash}"
        else:
            # ä½¿ç”¨æ ‡é¢˜hash
            title = paper.get('title', f'paper-{index}')
            title_hash = hashlib.md5(title.encode()).hexdigest()[:12]
            return f"paper-{title_hash}"

    def _generate_html_content(self, papers: List[Dict], research_interests: List[str], date_str: str, tweets: List[Dict] = None) -> str:
        """
        ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Šå†…å®¹

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            research_interests: ç ”ç©¶æ–¹å‘åˆ—è¡¨
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            tweets: æ¨æ–‡åˆ—è¡¨

        Returns:
            HTMLæ ¼å¼çš„æŠ¥å‘Šå†…å®¹
        """
        import html
        tweets = tweets or []

        # ä¸ºæ¯ç¯‡è®ºæ–‡ç”Ÿæˆå”¯ä¸€é”šç‚¹IDå¹¶å­˜å‚¨ï¼ˆå¼ºåˆ¶é‡æ–°ç”Ÿæˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
        for i, paper in enumerate(papers):
            paper['anchor_id'] = self._generate_paper_anchor(paper, i)

        # CSSæ ·å¼
        css_style = """
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;
                line-height: 1.6;
                color: #333;
                max-width: 900px;
                margin: 0 auto;
                padding: 20px;
                background-color: #f5f5f5;
            }
            .container {
                background-color: white;
                padding: 30px;
                border-radius: 8px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            h1 {
                color: #2c3e50;
                border-bottom: 3px solid #3498db;
                padding-bottom: 10px;
            }
            h2 {
                color: #34495e;
                border-bottom: 2px solid #ecf0f1;
                padding-bottom: 8px;
                margin-top: 30px;
            }
            h3 {
                color: #16a085;
                margin-top: 20px;
            }
            .stats {
                background-color: #ecf0f1;
                padding: 15px;
                border-radius: 5px;
                margin: 15px 0;
            }
            .stats ul {
                margin: 0;
                padding-left: 20px;
            }
            .summary {
                background-color: #e8f4f8;
                border-left: 4px solid #3498db;
                padding: 15px;
                margin: 20px 0;
            }
            .summary strong {
                color: #2980b9;
            }
            .paper {
                border: 1px solid #e0e0e0;
                border-radius: 5px;
                padding: 15px;
                margin: 15px 0;
                background-color: #fafafa;
            }
            .paper h3 {
                margin-top: 0;
                color: #2c3e50;
            }
            .paper-info {
                color: #666;
                font-size: 0.9em;
                margin: 5px 0;
            }
            .paper-abstract {
                background-color: white;
                padding: 10px;
                border-radius: 3px;
                margin: 10px 0;
                border-left: 3px solid #3498db;
            }
            .tweet {
                border: 1px solid #e1e8ed;
                border-radius: 5px;
                padding: 15px;
                margin: 15px 0;
                background-color: #f7f9fa;
            }
            .tweet-author {
                color: #1da1f2;
                font-weight: bold;
            }
            .tweet-content {
                margin: 10px 0;
                white-space: pre-wrap;
            }
            .cns-badge {
                background-color: #e74c3c;
                color: white;
                padding: 2px 8px;
                border-radius: 3px;
                font-size: 0.8em;
                font-weight: bold;
                margin-left: 5px;
            }
            .high-relevance {
                border-left: 4px solid #27ae60;
            }
            .medium-relevance {
                border-left: 4px solid #f39c12;
            }
            a {
                color: #3498db;
                text-decoration: none;
            }
            a:hover {
                text-decoration: underline;
            }
            .separator {
                border-top: 1px solid #ddd;
                margin: 30px 0;
            }
            .emoji {
                font-size: 1.2em;
            }
            .abstract-content {
                margin: 10px 0;
                padding: 10px;
                background-color: white;
                border-radius: 3px;
                line-height: 1.8;
                color: #333;
            }
        </style>
        """

        # HTMLå¼€å¤´
        html_parts = [
            "<!DOCTYPE html>",
            "<html lang='zh-CN'>",
            "<head>",
            "<meta charset='UTF-8'>",
            "<meta name='viewport' content='width=device-width, initial-scale=1.0'>",
            f"<title>å­¦æœ¯å†…å®¹æ—¥æŠ¥ - {date_str}</title>",
            css_style,
            "</head>",
            "<body>",
            "<div class='container'>"
        ]

        # æ ‡é¢˜
        html_parts.append(f"<h1>å­¦æœ¯å†…å®¹æ—¥æŠ¥ - {date_str}</h1>")

        # ç ”ç©¶æ–¹å‘
        html_parts.append("<h2>ç ”ç©¶æ–¹å‘</h2>")
        html_parts.append("<ul>")
        for interest in research_interests:
            html_parts.append(f"<li>{html.escape(interest)}</li>")
        html_parts.append("</ul>")

        # ç»Ÿè®¡ä¿¡æ¯
        html_parts.append("<div class='stats'>")
        html_parts.append("<h2>ç»Ÿè®¡ä¿¡æ¯</h2>")
        html_parts.append("<ul>")
        html_parts.append(f"<li>æ€»è®ºæ–‡/æ–‡ç« æ•°: {len(papers)}</li>")
        if tweets:
            html_parts.append(f"<li>æ€»æ¨æ–‡æ•°: {len(tweets)}</li>")

        # è®ºæ–‡ç›¸å…³æ€§ç»Ÿè®¡
        relevance_stats = {}
        for paper in papers:
            level = paper.get('relevance_level', 'unknown')
            if level != 'unknown':
                relevance_stats[level] = relevance_stats.get(level, 0) + 1

        if relevance_stats:
            html_parts.append("<li>è®ºæ–‡ç›¸å…³æ€§åˆ†å¸ƒ:<ul>")
            for level in ['high', 'medium', 'low', 'none']:
                if level in relevance_stats:
                    html_parts.append(f"<li>{level}: {relevance_stats[level]}</li>")
            html_parts.append("</ul></li>")

        # æ¨æ–‡ç›¸å…³æ€§ç»Ÿè®¡
        if tweets:
            tweet_stats = {}
            for tweet in tweets:
                level = tweet.get('relevance_level', 'unknown')
                if level != 'unknown':
                    tweet_stats[level] = tweet_stats.get(level, 0) + 1
            if tweet_stats:
                html_parts.append("<li>æ¨æ–‡ç›¸å…³æ€§åˆ†å¸ƒ:<ul>")
                for level in ['high', 'medium', 'low']:
                    if level in tweet_stats:
                        html_parts.append(f"<li>{level}: {tweet_stats[level]}</li>")
                html_parts.append("</ul></li>")

        html_parts.append("</ul>")
        html_parts.append("</div>")

        # æ ¸å¿ƒå‘ç°æ‘˜è¦
        high_papers = [p for p in papers if p.get('relevance_level') == 'high']
        high_tweets = [t for t in tweets if t.get('relevance_level') == 'high']

        if high_papers or high_tweets:
            html_parts.append("<div class='summary'>")
            html_parts.append("<h2><span class='emoji'>ğŸ”</span> æ ¸å¿ƒå‘ç°é€Ÿè§ˆ</h2>")

            # è®ºæ–‡æ‘˜è¦
            if high_papers:
                summary_html = self._generate_topic_summary_html(high_papers)
                html_parts.append(summary_html)

            # Twitteræ‘˜è¦
            if high_tweets:
                twitter_summary_html = self._generate_twitter_summary_html(high_tweets)
                html_parts.append(twitter_summary_html)

            html_parts.append("</div>")

        # åˆ†ç»„è®ºæ–‡
        medium_papers = [p for p in papers if p.get('relevance_level') == 'medium']
        low_papers = [p for p in papers if p.get('relevance_level') == 'low']

        # é«˜ç›¸å…³è®ºæ–‡
        if high_papers:
            html_parts.append("<h2>å¼ºçƒˆæ¨è (é«˜ç›¸å…³æ€§)</h2>")
            for paper in high_papers:
                html_parts.append(self._format_paper_html(paper, 'high'))

        # ä¸­ç­‰ç›¸å…³è®ºæ–‡
        if medium_papers:
            html_parts.append("<h2>æ¨èé˜…è¯» (ä¸­ç­‰ç›¸å…³æ€§)</h2>")
            for paper in medium_papers:
                html_parts.append(self._format_paper_html(paper, 'medium'))

        # Twitteræ¨æ–‡
        if tweets:
            html_parts.append("<div class='separator'></div>")
            html_parts.append("<h1><span class='emoji'>ğŸ“±</span> Twitter å­¦æœ¯åŠ¨æ€</h1>")

            high_tweets_list = [t for t in tweets if t.get('relevance_level') == 'high']
            medium_tweets_list = [t for t in tweets if t.get('relevance_level') == 'medium']

            if high_tweets_list:
                html_parts.append("<h2>å¼ºçƒˆæ¨è (é«˜ç›¸å…³æ€§)</h2>")
                for tweet in high_tweets_list:
                    html_parts.append(self._format_tweet_html(tweet))

            if medium_tweets_list:
                html_parts.append("<h2>å€¼å¾—å…³æ³¨ (ä¸­ç­‰ç›¸å…³æ€§)</h2>")
                for tweet in medium_tweets_list:
                    html_parts.append(self._format_tweet_html(tweet))

        # HTMLç»“å°¾
        html_parts.append("</div>")
        html_parts.append("</body>")
        html_parts.append("</html>")

        return '\n'.join(html_parts)

    def _generate_topic_summary_html(self, papers: List[Dict]) -> str:
        """ç”Ÿæˆè®ºæ–‡ä¸»é¢˜æ‘˜è¦çš„HTML"""
        import html
        import re

        parts = []
        parts.append(f"<p>æ ¹æ®æ‚¨çš„ç ”ç©¶å…´è¶£ï¼Œæœ¬æœŸä¸ºæ‚¨ç­›é€‰å‡º <strong>{len(papers)}ç¯‡é«˜ç›¸å…³è®ºæ–‡</strong>ã€‚ä»¥ä¸‹æ˜¯æ ¸å¿ƒå‘ç°ï¼š</p>")

        # æŒ‰ä¸»é¢˜åˆ†ç±»
        topic_papers = {}
        for paper in papers:
            interests = paper.get('matched_interests', [])
            source = paper.get('journal', paper.get('primary_category', 'ArXiv'))
            is_cns = any(journal in str(source) for journal in ['Nature', 'Science', 'Cell'])

            if interests:
                for interest in interests:
                    if interest not in topic_papers:
                        topic_papers[interest] = []
                    topic_papers[interest].append((paper, is_cns))

        total_recommended = 0
        max_total = 10

        for topic, topic_papers_list in sorted(topic_papers.items(), key=lambda x: -len(x[1])):
            if total_recommended >= max_total:
                break

            topic_papers_list.sort(key=lambda x: (not x[1], x[0].get('title', '')))
            recommend_count = min(3, len(topic_papers_list), max_total - total_recommended)

            parts.append(f"<p><strong>{html.escape(topic)}</strong>ï¼šæœ¬æœŸæ‰¾åˆ°{len(topic_papers_list)}ç¯‡ç›¸å…³è®ºæ–‡ï¼Œä¸ºæ‚¨é‡ç‚¹æ¨èä»¥ä¸‹{recommend_count}ç¯‡ï¼š</p>")
            parts.append("<ol>")

            for paper, is_cns in topic_papers_list[:recommend_count]:
                total_recommended += 1
                title = html.escape(paper.get('title', 'æœªçŸ¥æ ‡é¢˜'))
                source = paper.get('journal') if paper.get('journal') else f"ArXiv {paper.get('primary_category', 'unknown')}"

                # ä½¿ç”¨é¢„å…ˆç”Ÿæˆçš„é”šç‚¹ID
                anchor = paper.get('anchor_id', 'unknown')

                cns_badge = '<span class="cns-badge">CNS</span>' if is_cns else ''
                brief = paper.get('summary', paper.get('why_relevant', ''))
                if brief:
                    brief = html.escape(brief.split('ã€‚')[0].split('.')[0][:100].strip())

                parts.append(f"<li><a href='#{anchor}'>{title}</a> ({html.escape(source)}) {cns_badge}")
                if brief:
                    parts.append(f"<br><small>{brief}</small>")
                parts.append("</li>")

            parts.append("</ol>")

            remaining = len(topic_papers_list) - recommend_count
            if remaining > 0:
                parts.append(f"<p><em>å¦æœ‰{remaining}ç¯‡{html.escape(topic)}ç›¸å…³è®ºæ–‡ï¼Œè¯¦è§ä¸‹æ–‡</em></p>")

        if total_recommended < len(papers):
            parts.append(f"<p><em>å…¶ä½™ {len(papers) - total_recommended} ç¯‡é«˜ç›¸å…³è®ºæ–‡è¯¦è§ä¸‹æ–‡ã€Œå¼ºçƒˆæ¨èã€éƒ¨åˆ†ã€‚</em></p>")

        return '\n'.join(parts)

    def _generate_twitter_summary_html(self, tweets: List[Dict]) -> str:
        """ç”ŸæˆTwitteræ‘˜è¦çš„HTML"""
        import html

        parts = []
        parts.append("<p><strong><span class='emoji'>ğŸ“±</span> Twitterå­¦æœ¯åŠ¨æ€</strong></p>")

        # åˆ†æä¸»é¢˜
        topics = {}
        topic_keywords = {
            'è‡ªåŠ¨é©¾é©¶': ['è‡ªåŠ¨é©¾é©¶', 'autonomous driving', 'self-driving', 'Tesla', 'FSD'],
            'å¤§è¯­è¨€æ¨¡å‹': ['LLM', 'GPT', 'Claude', 'language model', 'å¤§è¯­è¨€æ¨¡å‹', 'å¤§æ¨¡å‹'],
            'å¼ºåŒ–å­¦ä¹ ': ['å¼ºåŒ–å­¦ä¹ ', 'reinforcement learning', 'RL', 'policy'],
            'å…·èº«æ™ºèƒ½': ['å…·èº«', 'embodied', 'robot', 'æœºå™¨äºº', 'manipulation'],
            'VLM/å¤šæ¨¡æ€': ['VLM', 'vision language', 'å¤šæ¨¡æ€', 'multimodal', 'CLIP'],
        }

        for tweet in tweets:
            text = tweet.get('text', '')
            for topic, kws in topic_keywords.items():
                if any(kw.lower() in text.lower() for kw in kws):
                    if topic not in topics:
                        topics[topic] = []
                    topics[topic].append(tweet)
                    break

        if topics:
            topic_summary = [f"<strong>{html.escape(topic)}</strong>ï¼ˆ{len(tweets)}æ¡ï¼‰"
                           for topic, tweets in sorted(topics.items(), key=lambda x: -len(x[1]))[:3]]
            parts.append(f"<p>æœ¬æœŸTwitterå­¦æœ¯åœˆçš„çƒ­ç‚¹è¯é¢˜åŒ…æ‹¬ï¼š{' | '.join(topic_summary)}</p>")

        sample_count = min(3, len(tweets))
        parts.append(f"<p>ä¸ºæ‚¨ç²¾é€‰{sample_count}æ¡æœ€å…·ä»£è¡¨æ€§çš„è®¨è®ºï¼š</p>")
        parts.append("<ol>")

        for tweet in tweets[:sample_count]:
            author = html.escape(tweet.get('author_name', tweet.get('author_username', 'æœªçŸ¥')))
            text_preview = html.escape(tweet.get('text', '')[:100].strip())
            if len(tweet.get('text', '')) > 100:
                text_preview += '...'

            parts.append(f"<li><strong>@{author}</strong>: {text_preview}")
            if tweet.get('why_relevant'):
                relevance = html.escape(tweet.get('why_relevant', '')[:80])
                parts.append(f"<br><em>{relevance}</em>")
            parts.append("</li>")

        parts.append("</ol>")
        parts.append("<p><em>å®Œæ•´TwitteråŠ¨æ€è¯¦è§ä¸‹æ–‡ã€ŒTwitterå­¦æœ¯åŠ¨æ€ã€éƒ¨åˆ†</em></p>")

        return '\n'.join(parts)

    def _format_paper_html(self, paper: Dict, relevance: str = 'medium') -> str:
        """æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡ä¸ºHTML"""
        import html
        import re
        import urllib.parse

        # ä½¿ç”¨é¢„å…ˆç”Ÿæˆçš„é”šç‚¹ID
        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
        anchor = paper.get('anchor_id', 'unknown')

        relevance_class = f"{relevance}-relevance"

        parts = [f"<div class='paper {relevance_class}' id='{anchor}'>"]
        parts.append(f"<h3>{html.escape(title)}</h3>")

        # åŸºæœ¬ä¿¡æ¯
        published = paper.get('published') or paper.get('pub_date') or paper.get('date', 'æœªçŸ¥')
        parts.append(f"<div class='paper-info'><strong>å‘å¸ƒæ—¥æœŸ:</strong> {html.escape(str(published))}</div>")

        # ä½œè€…
        authors = paper.get('authors', [])
        if authors:
            authors_display = ', '.join(authors[:3])
            if len(authors) > 3:
                authors_display += f" ç­‰ ({len(authors)}ä½ä½œè€…)"
            parts.append(f"<div class='paper-info'><strong>ä½œè€…:</strong> {html.escape(authors_display)}</div>")

        # ä½œè€…å•ä½ï¼ˆå¦‚æœæœ‰ï¼‰
        if paper.get('affiliations'):
            parts.append(f"<div class='paper-info'><strong>å•ä½:</strong> {html.escape(paper['affiliations'])}</div>")

        # æœŸåˆŠ/ç±»åˆ«
        if paper.get('journal'):
            journal = paper['journal']
            is_cns = any(j in journal for j in ['Nature', 'Science', 'Cell'])
            cns_badge = '<span class="cns-badge">CNS</span>' if is_cns else ''
            parts.append(f"<div class='paper-info'><strong>æœŸåˆŠ:</strong> {html.escape(journal)} {cns_badge}</div>")
        elif paper.get('primary_category'):
            parts.append(f"<div class='paper-info'><strong>ç±»åˆ«:</strong> {html.escape(paper['primary_category'])}</div>")

        # ç›¸å…³é¢†åŸŸ
        if paper.get('matched_interests'):
            interests = ', '.join(paper['matched_interests'])
            parts.append(f"<div class='paper-info'><strong>ç›¸å…³é¢†åŸŸ:</strong> {html.escape(interests)}</div>")

        # é“¾æ¥
        if paper.get('url'):
            parts.append(f"<div class='paper-info'><strong>è®ºæ–‡é“¾æ¥:</strong> <a href='{html.escape(paper['url'])}' target='_blank'>{html.escape(paper['url'])}</a></div>")
        if paper.get('pdf_url'):
            parts.append(f"<div class='paper-info'><strong>PDFé“¾æ¥:</strong> <a href='{html.escape(paper['pdf_url'])}' target='_blank'>{html.escape(paper['pdf_url'])}</a></div>")


        # ç”Ÿæˆå”¯ä¸€IDï¼ˆç”¨äºå±•å¼€/æŠ˜å åŠŸèƒ½ï¼‰
        paper_id = anchor

        # æ‘˜è¦æ˜¾ç¤ºæ ¼å¼ï¼šæ ¸å¿ƒå†…å®¹ï¼ˆä¸€å¥è¯ï¼‰+ å®Œæ•´æ‘˜è¦
        if paper.get('abstract_zh') or paper.get('summary'):
            parts.append("<div class='paper-abstract'>")

            # AIç”Ÿæˆçš„æ ¸å¿ƒå†…å®¹ï¼ˆæ˜¾ç¤ºç¬¬ä¸€å¥è¯ä½œä¸ºæ€»ç»“ï¼‰
            if paper.get('summary'):
                summary = paper['summary']
                # æå–ç¬¬ä¸€å¥è¯ä½œä¸ºæ ¸å¿ƒå†…å®¹
                first_sentence = summary.split('ã€‚')[0]
                if not first_sentence.endswith('ã€‚'):
                    first_sentence = first_sentence.split('.')[0]
                # é™åˆ¶é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                if len(first_sentence) > 200:
                    first_sentence = first_sentence[:200] + '...'

                parts.append(f"<strong>æ ¸å¿ƒå†…å®¹:</strong>")
                parts.append(f"<div class='abstract-content'>{html.escape(first_sentence)}...</div>")

            # ä¸­æ–‡æ‘˜è¦ï¼ˆæ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼‰
            if paper.get('abstract_zh'):
                abstract_zh = paper['abstract_zh']
                parts.append(f"<strong>æ‘˜è¦:</strong>")
                parts.append(f"<div class='abstract-content'>{html.escape(abstract_zh)}</div>")

            parts.append("</div>")

        parts.append("</div>")
        return '\n'.join(parts)

    def _format_tweet_html(self, tweet: Dict) -> str:
        """æ ¼å¼åŒ–å•æ¡æ¨æ–‡ä¸ºHTML"""
        import html

        parts = ["<div class='tweet'>"]

        # ä½œè€…
        author = f"@{tweet.get('author_username', 'unknown')}"
        if tweet.get('author_name'):
            author = f"{tweet.get('author_name')} ({author})"
        parts.append(f"<div class='tweet-author'>{html.escape(author)}</div>")

        # æ—¶é—´
        parts.append(f"<div class='paper-info'>{html.escape(str(tweet.get('created_at', 'unknown')))}</div>")

        # å†…å®¹
        parts.append(f"<div class='tweet-content'>{html.escape(tweet.get('text', ''))}</div>")

        # ç›¸å…³æ€§è¯´æ˜
        if tweet.get('why_relevant'):
            parts.append(f"<div class='paper-info'><strong>ç›¸å…³æ€§:</strong> {html.escape(tweet['why_relevant'])}</div>")

        # äº’åŠ¨æ•°æ®
        if tweet.get('favorite_count') or tweet.get('retweet_count'):
            parts.append(f"<div class='paper-info'>ğŸ‘ {tweet.get('favorite_count', 0)} | ğŸ”„ {tweet.get('retweet_count', 0)} | ğŸ’¬ {tweet.get('reply_count', 0)}</div>")

        # é“¾æ¥
        if tweet.get('url'):
            parts.append(f"<div class='paper-info'><a href='{html.escape(tweet['url'])}' target='_blank'>æŸ¥çœ‹æ¨æ–‡</a></div>")

        parts.append("</div>")
        return '\n'.join(parts)
